{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto refering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import nltk.data\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer, WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT = 'example_texts.json'\n",
    "INPUT = 'dataset_43428_1.txt'\n",
    "OUTPUT = 'referats.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'А бойтесь единственно только того , кто скажет :\\n« Я знаю , как надо ! »\\nСтивен Хант использует стилистику « парового панка » , для того чтобы поговорить об актуальных проблемах современности\\n\\nСтимпанк , он же « паровой панк » , — это прежде всего антураж : субмарины на паровом ходу , аэростаты , наполненные летучим газом , дамы в кринолинах и мужчины с тросточками .\\nА ещё это атмосфера : затхлая безнадёга работных домов , где гнут спины юные Оливеры Твисты , готика сельских кладбищ , скрежет шестерёнок и кривошипов в недрах гигантских дифференциальных исчислителей ...\\nКонан Дойл пополам с Диккенсом плюс капелька кафкианского абсурда и лавкрафтовского ужаса : викторианский мир , резко шагнувший в эпоху НТР , но не преодолевший противоречия , разрывающие сословное общество изнутри .\\nВсё это есть в романе Стивена Ханта , а также подземный город , основанный изгоями , живущими надеждой на пролетарскую революцию , разумные роботы - паровики , мутанты , наделённые сверхъестественными способностями , жуткие древние боги , готовые вот-вот вырваться на свободу , и много чего ещё .\\n\\nПредставители движения « новых странных » ( new weird ) , набирающего обороты в Британии и США с начала 2000-х годов , давно предлагают использовать инструментарий фэнтези не для создания миров , куда обыватель бежит от свинцовых мерзостей быта , а для обсуждения насущных социальных проблем .\\nСтивен Хант отчасти поддерживает эту позицию , отчасти оппонирует своим коллегам по цеху .\\nЕго мир не менее эклектичен и причудлив , чем у авторов , принадлежащих к ядру new weird .\\nОднако левые политические взгляды , характерные , скажем , для Чайны Мьевилля и ( в меньшей степени ) для Джеффа Вандермеера , чужды автору « Небесного суда » : кровавые « перегибы » , сопровождающие любую революцию , он живописует не менее азартно , чем родимые пятна раннебуржуазного общества .\\nГерои романа , двое необычных подростков , принадлежащих к разным сословиям , должны спасти мир от пробуждения древних демонов , а свою родную страну — от торжества лицемерных « уравнивателей » , с одинаковым усердием истребляющих старую аристократию и членов парламента , художников и азартных игроков .\\nВ отличие от « новых странных » , культивирующих сложный , вычурный стиль , Стивен Хант адресует свой роман сверстникам героев книги .\\nПисатель не чурается натуралистических сцен и описаний , однако язык « Небесного суда » прост и прозрачен , а главный месседж предельно отчётлив : любая Большая Идея , победив , стремится назначить себя единственно верной истиной .\\nА « единственно верная истина » так и норовит погубить — человека , страну , весь мир .\\nТак что к любой Большой Идее следует относиться с подозрением , даже если перед нами светлая идея всеобщего равенства и братства .\\nВот так по-галичевски :\\n« Не бойтесь тюрьмы , не бойтесь сумы , не бойтесь мора и глада , а бойтесь единственно только того , кто скажет :\\n« Я знаю , как надо ! »\\nНе бог весть как оригинально , зато внятно и доходчиво — у Ханта есть неплохой шанс победить в этом споре своих оппонентов , запутавшихся в противоречивых тезисах .\\n\\nБлогеры :\\n\\nАндрей Валентинович ( Лаборатория фантастики ) :\\n\\n« Прочитав примерно треть романа могу сказать — не знаю , как насчёт Вэнса , но к Диккенсу С . Хант имеет примерно такое же отношение , как Дарья Донцова к Агате Кристи .\\nИли романы Александра Варго к Стивену Кингу .\\n\\nДа , Хант использует в своём романе псевдовикторианский антураж — работные дома , городские трущобы , притоны и т . п .\\nНе более не менее .\\n\\nПравда , весь этот антураж выглядит в романе Ханта удивительно скучным и тривиальным .\\n\\nСтоль же скучными и предсказуемыми оказываются и главные герои — Очень Хороший Мальчик ( Оливер ) и Очень Хорошая Девочка ( Молли ) , наделённые паранормальными способностями из-за которых подростков преследуют Очень Жестокие Негодяи .\\nДети спасаются от негодяев параллельными маршрутами , но рано или поздно их пути пересекутся ...\\n\\nВ результате мы имеем Очень Средний по своим литературным достоинствам Текст — герои , не вызывающие ни малейшего интереса ; вымученная викторианская атмосфера , которую Хант не слишком хорошо знает , да и просто не умеет передать .\\n\\nЗато есть говорящие паровые машины , подземные города , кассарабийская магия и прочие плоды авторской фантазии .\\nНо всё равно получается невыносимо скучно .\\nВообщем , продолжать это унылое и безсмысленное чтение я не собираюсь » .\\n\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = json.load(open(INPUT, 'r'))\n",
    "SAMPLE_TEXT = texts[0]\n",
    "SAMPLE_TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24694"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(len(text) for text in texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3027.31"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(len(text) for text in texts) / len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'А бойтесь единственно только того , кто скажет :\\n« Я знаю , как надо !'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(SAMPLE_TEXT)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_referat_sent(text):\n",
    "    return sent_tokenize(text)[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_referat_prefix(text):\n",
    "    return text[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSummarizer:\n",
    "    def summarize(self, input, num_sentences=3):\n",
    "        return \" \".join(self._get_summarized(input, num_sentences))\n",
    "    \n",
    "    def _reorder_sentences(self, output_sentences, input):\n",
    "        output_sentences.sort(key=input.find)\n",
    "        return output_sentences\n",
    "    \n",
    "    def _get_summarized(self, input, num_sentences):\n",
    "        tokenizer = RegexpTokenizer('\\w+')\n",
    "        \n",
    "        # get the frequency of each word in the input\n",
    "        base_words = [word.lower() for word in tokenizer.tokenize(input)]\n",
    "        words = [word for word in base_words if word not in stopwords.words()]\n",
    "        word_frequencies = FreqDist(words)\n",
    "        \n",
    "        # now create a set of the most frequent words\n",
    "        most_frequent_words = [w for _, w in sorted((c, w) for w, c in word_frequencies.items())[:100]]\n",
    "        \n",
    "        # break the input up into sentences.  working_sentences is used\n",
    "        # for the analysis, but actual_sentences is used in the results\n",
    "        # so capitalization will be correct.\n",
    "        \n",
    "        sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "        actual_sentences = sent_detector.tokenize(input)\n",
    "        working_sentences = [sentence.lower() for sentence in actual_sentences]\n",
    "        \n",
    "        # iterate over the most frequent words, and add the first sentence\n",
    "        # that inclues each word to the result.\n",
    "        output_sentences = []\n",
    "        for word in most_frequent_words:\n",
    "            for i in range(0, len(working_sentences)):\n",
    "                if (word in working_sentences[i] and actual_sentences[i] not in output_sentences):\n",
    "                    output_sentences.append(actual_sentences[i])\n",
    "                    break\n",
    "                if len(output_sentences) >= num_sentences: break\n",
    "            if len(output_sentences) >= num_sentences: break\n",
    "            \n",
    "        # sort the output sentences back to their original order\n",
    "        return self._reorder_sentences(output_sentences, input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, pdb, sys, math\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class Graph:\n",
    "\tdef __init__(self):\n",
    "\t\tself.Vertices = []\n",
    "\t\tself.Edges = []\n",
    "\n",
    "\tdef getRankedVertices(self):\n",
    "\t\tres = defaultdict(float)\n",
    "\t\tfor e in self.Edges:\n",
    "\t\t\tres[e.Vertex1] += e.Weight\n",
    "\t\treturn sorted(res.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "class Vertex:\n",
    "\tdef __init__(self):\n",
    "\t\tself.Sentence = None\n",
    "\n",
    "class Edge:\n",
    "\tdef __init__(self):\n",
    "\t\tself.Vertex1 = None\n",
    "\t\tself.Vertex2 = None\n",
    "\t\tself.Weight = 0\n",
    "\n",
    "class WordType:\n",
    "\tContent=0\n",
    "\tFunction=1\n",
    "\tContentPunctuation=2\n",
    "\tFunctionPunctuation=3\n",
    "\n",
    "class Word:\n",
    "\tdef __init__(self):\n",
    "\t\tself.Text=''\n",
    "\t\tself.Type=''\n",
    "\n",
    "class Sentence:\n",
    "\tdef __init__(self):\n",
    "\t\tself.Words = []\n",
    "\n",
    "\tdef getFullSentence(self):\n",
    "\t\ttext = ''\n",
    "\t\tfor w in self.Words:\n",
    "\t\t\ttext += w.Text\n",
    "\t\treturn text.strip()\n",
    "\n",
    "\tdef getReducedSentence(self):\n",
    "\t\tsentenceText = ''\n",
    "\t\tsentenceEnd = self.Words[len(self.Words)-1]\n",
    "\t\tcontentWords = list(filter(lambda w: w.Type == WordType.Content, self.Words))\n",
    "\t\ti = 0\n",
    "\t\twhile i < len(contentWords):\n",
    "\t\t\tw = contentWords[i]\n",
    "\t\t\t# upper case the first character of the sentence\n",
    "\t\t\tif i == 0:\n",
    "\t\t\t\tli = list(w.Text)\n",
    "\t\t\t\tli[0] = li[0].upper()\n",
    "\t\t\t\tw.Text = ''.join(li)\n",
    "\t\t\tsentenceText += w.Text\n",
    "\t\t\tif i < len(contentWords)-1:\n",
    "\t\t\t\tsentenceText += ' '\n",
    "\t\t\telif sentenceEnd.Text != w.Text:\n",
    "\t\t\t\tsentenceText += sentenceEnd.Text\n",
    "\t\t\ti = i+1\n",
    "\t\treturn sentenceText\n",
    "\t\t\t\n",
    "\n",
    "class Paragraph:\n",
    "\tdef __init__(self):\n",
    "\t\tself.Sentences = []\n",
    "\n",
    "class Reduction:\n",
    "\tfunctionPunctuation = ' ,-'\n",
    "\tcontentPunctuation = '.?!\\n'\n",
    "\tpunctuationCharacters = functionPunctuation+contentPunctuation\n",
    "\tsentenceEndCharacters = '.?!'\n",
    "\t\n",
    "\tdef isContentPunctuation(self, text):\n",
    "\t\tfor c in self.contentPunctuation:\n",
    "\t\t\tif text.lower() == c.lower():\n",
    "\t\t\t\treturn True\n",
    "\t\treturn False\n",
    "\n",
    "\tdef isFunctionPunctuation(self, text):\n",
    "\t\tfor c in self.functionPunctuation:\n",
    "\t\t\tif text.lower() == c.lower():\n",
    "\t\t\t\treturn True\n",
    "\t\treturn False\n",
    "\n",
    "\tdef isFunction(self, text, stopWords):\n",
    "\t\tfor w in stopWords:\n",
    "\t\t\tif text.lower() == w.lower():\n",
    "\t\t\t\treturn True\n",
    "\t\treturn False\n",
    "\n",
    "\tdef tag(self, sampleWords, stopWords):\n",
    "\t\ttaggedWords = []\n",
    "\t\tfor w in sampleWords:\n",
    "\t\t\ttw = Word()\n",
    "\t\t\ttw.Text = w\n",
    "\t\t\tif self.isContentPunctuation(w):\n",
    "\t\t\t\ttw.Type = WordType.ContentPunctuation\n",
    "\t\t\telif self.isFunctionPunctuation(w):\n",
    "\t\t\t\ttw.Type = WordType.FunctionPunctuation\n",
    "\t\t\telif self.isFunction(w, stopWords):\n",
    "\t\t\t\ttw.Type = WordType.Function\n",
    "\t\t\telse:\n",
    "\t\t\t\ttw.Type = WordType.Content\n",
    "\t\t\ttaggedWords.append(tw)\n",
    "\t\treturn taggedWords\n",
    "\n",
    "\tdef tokenize(self, text):\n",
    "\t\treturn list(filter(lambda w: w != '', re.split('([{0}])'.format(self.punctuationCharacters), text)))\n",
    "\n",
    "\tdef getWords(self, sentenceText, stopWords):\n",
    "\t\treturn self.tag(self.tokenize(sentenceText), stopWords) \n",
    "\n",
    "\tdef getSentences(self, line, stopWords):\n",
    "\t\tsentences = []\n",
    "\t\tsentenceTexts = list(filter(lambda w: w.strip() != '', re.split('[{0}]'.format(self.sentenceEndCharacters), line)))\n",
    "\t\tsentenceEnds = re.findall('[{0}]'.format(self.sentenceEndCharacters), line)\n",
    "\t\tsentenceEnds.reverse()\n",
    "\t\tfor t in sentenceTexts:\n",
    "\t\t\tif len(sentenceEnds) > 0:\n",
    "\t\t\t\tt += sentenceEnds.pop()\n",
    "\t\t\tsentence = Sentence()\n",
    "\t\t\tsentence.Words = self.getWords(t, stopWords)\n",
    "\t\t\tsentences.append(sentence)\n",
    "\t\treturn sentences\n",
    "\n",
    "\tdef getParagraphs(self, lines, stopWords):\n",
    "\t\tparagraphs = []\n",
    "\t\tfor line in lines:\n",
    "\t\t\tparagraph = Paragraph()\n",
    "\t\t\tparagraph.Sentences = self.getSentences(line, stopWords)\n",
    "\t\t\tparagraphs.append(paragraph)\n",
    "\t\treturn paragraphs\n",
    "\n",
    "\tdef findWeight(self, sentence1, sentence2):\n",
    "\t\tlength1 = len(list(filter(lambda w: w.Type == WordType.Content, sentence1.Words)))\n",
    "\t\tlength2 = len(list(filter(lambda w: w.Type == WordType.Content, sentence2.Words)))\n",
    "\t\tif length1 < 4 or length2 < 4:\n",
    "\t\t\treturn 0\n",
    "\t\tweight = 0\n",
    "\t\tfor w1 in list(filter(lambda w: w.Type == WordType.Content, sentence1.Words)):\n",
    "\t\t\tfor w2 in list(filter(lambda w: w.Type == WordType.Content, sentence2.Words)):\n",
    "\t\t\t\tif w1.Text.lower() == w2.Text.lower():\n",
    "\t\t\t\t\tweight = weight + 1\n",
    "\t\tnormalised1 = 0\n",
    "\t\tif length1 > 0:\n",
    "\t\t\tnormalised1 = math.log(length1)\n",
    "\t\tnormalised2 = 0\n",
    "\t\tif length2 > 0:\n",
    "\t\t\tnormalised2 = math.log(length2)\n",
    "\t\tnorm = normalised1 + normalised2\n",
    "\t\tif norm == 0:\n",
    "\t\t\treturn 0\n",
    "\t\treturn weight / float(norm)\n",
    "\n",
    "\tdef buildGraph(self, sentences):\n",
    "\t\tg = Graph()\n",
    "\t\tfor s in sentences:\n",
    "\t\t\tv = Vertex()\n",
    "\t\t\tv.Sentence = s\n",
    "\t\t\tg.Vertices.append(v)\n",
    "\t\tfor i in g.Vertices:\n",
    "\t\t\tfor j in g.Vertices:\n",
    "\t\t\t\tif i != j:\n",
    "\t\t\t\t\tw = self.findWeight(i.Sentence, j.Sentence)\n",
    "\t\t\t\t\te = Edge()\n",
    "\t\t\t\t\te.Vertex1 = i\n",
    "\t\t\t\t\te.Vertex2 = j\n",
    "\t\t\t\t\te.Weight = w\n",
    "\t\t\t\t\tg.Edges.append(e)\n",
    "\t\treturn g\n",
    "\n",
    "\tdef sentenceRank(self, paragraphs):\n",
    "\t\tsentences = []\n",
    "\t\tfor p in paragraphs:\n",
    "\t\t\tfor s in p.Sentences:\n",
    "\t\t\t\tsentences.append(s)\n",
    "\t\tg = self.buildGraph(sentences)\n",
    "\t\treturn g.getRankedVertices()\n",
    "\n",
    "\tdef reduce(self, text, reductionRatio):\n",
    "\t\tstopWords= stopwords.words()\n",
    "\n",
    "\t\tlines = text.splitlines()\n",
    "\t\tcontentLines = list(filter(lambda w: w.strip() != '', lines))\n",
    "\n",
    "\t\tparagraphs = self.getParagraphs(contentLines, stopWords)\n",
    "\n",
    "\t\trankedSentences = self.sentenceRank(paragraphs)\n",
    "\n",
    "\t\torderedSentences = []\n",
    "\t\tfor p in paragraphs:\n",
    "\t\t\tfor s in p.Sentences:\n",
    "\t\t\t\torderedSentences.append(s)\n",
    "\n",
    "\t\treducedSentences = []\n",
    "\t\ti = 0\n",
    "\t\twhile i < math.trunc(len(rankedSentences) * reductionRatio):\n",
    "\t\t\ts = rankedSentences[i][0].Sentence\n",
    "\t\t\tposition = orderedSentences.index(s)\n",
    "\t\t\treducedSentences.append((s, position))\n",
    "\t\t\ti = i + 1\n",
    "\t\treducedSentences = sorted(reducedSentences, key=lambda x: x[1])\n",
    "\t\t\n",
    "\t\treducedText = []\n",
    "\t\tfor s,r in reducedSentences:\n",
    "\t\t\treducedText.append(s.getFullSentence())\n",
    "\t\treturn reducedText\t\n",
    "\n",
    "    \n",
    "def reduct_reducer(text):\n",
    "    n = len(text)\n",
    "    if n <= 350:\n",
    "        return text\n",
    "    else:\n",
    "        return ' '.join(Reduction().reduce(text, 350 / n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.utils import get_stop_words\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "\n",
    "\n",
    "def summarize_sumy(text):\n",
    "    summarizer = Summarizer(Stemmer('russian'))\n",
    "    summarizer.stop_words = stopwords.words()\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "    return ' '.join(str(s) for s in summarizer(parser.document, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graph'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-104-3bd6224d1309>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msumma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msummarizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msummarize_summa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp-hw-v0btboDg/lib/python3.6/site-packages/summa/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msumma\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcommons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpagerank_weighted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m                   \u001b[0msummarizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msyntactic_unit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtextrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtextrank_runtime_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/nlp-hw-v0btboDg/lib/python3.6/site-packages/summa/commons.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbuild_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graph'"
     ]
    }
   ],
   "source": [
    "from summa import summarizer\n",
    "\n",
    "\n",
    "def summarize_summa(text):\n",
    "    n = len(text)\n",
    "    if n <= 350:\n",
    "        return text\n",
    "    else:\n",
    "        return summarizer.summarize(text, ratio=350 / n, language='russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_referer = summarize_sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Митрополит Иларион призвал поклонников Сталина отрезвиться и съездить на Бутовский полигон\\nПредседатель ОВЦС предложил тем , кто оправдывает сталинские репрессии , ответить на вопрос , за что расстреливали 15-летних детей , и заявил , что таким преступлениям не может быть никакого оправдания .\\n\\nГлава Отдела внешних церковных связей Московского патриархата , митрополит Волоколамский Иларион ( Алфеев ) gрокомментировал заявления социологов о росте популярности Иосифа Сталина у жителей России .\\nПо его мнению , тем , кто считает Сталина выдающимся деятелем , следует &quot; отрезвиться &quot; .\\nМитрополит Иларион предложил , оценивая роль Сталина в истории , учитывать два аспекта .\\n&quot; Мы знаем , что есть историческая заслуга всех тех , кто со стороны нашего государства вел войну , чтобы мы победили , а не проиграли , и этой заслуги нельзя умалять .\\nНо то , что были многомиллионные жертвы , репрессии , то , что был геноцид собственного населения , - это все было .\\nМы не можем , не должны закрывать на это глаза .\\nЯ думаю , что для того , чтобы кому-то отрезвиться , надо съездить на Бутовский полигон на окраине Москвы .\\nТам есть музей , фотографии людей , там рассказывают что происходило : каждый день привозили и расстреливали ночью по 200 - 300 - 400 человек &quot; ,\\n- цитирует NEWSru.com владыку Илариона .\\n\\nСановник рассказал о том , как лично знакомился со списками расстрелянных на Бутовском полигоне людей .\\n&quot; Там были 15- 16-летние дети .\\nЗа что их расстреливали ?\\nКакой вред мог принести государству 15-летний ребенок , чтобы его надо было расстрелять вот так вот исподтишка , тайно ?\\nЧудом Божиим сейчас восстановили этот мемориал , и мы знаем тех , кто там погиб .\\nЯ думаю , что этим преступлениям чудовищным оправдания никакого нет .\\nИ быть не может &quot; ,\\n- подчеркнул митрополит Волоколамский .\\n\\nВ ходе передачи он положительно оценил отношения , сложившиеся на текущиймомент между Церковью и государством , отметив два основных принципа : невмешательство Церкви и государства во внутренние дела друг друга и соработничество там , где это кажется полезным обеим сторонам .\\nМитрополит Иларион отметил , что &quot; у Церкви появилась такая свобода , какую она не имела никогда прежде - ни в советское время , ни в дореволюционное &quot; .\\nЭто , по его словам , стало &quot; одним из самых главных вызовов &quot; для РПЦ .\\n\\nЕще одним вызовом он назвал &quot; потребительскую идеологию , которая сегодня навязывается людям через СМИ , через систему образования &quot; .\\n&quot; У Церкви есть установившиеся от самого Христа заповеди в области нравственности , которые она считает незыблемыми ,\\n- заметил он .\\n– И не все они совпадают с современными трендами , поэтому часто приходится говорить что-то , что идет вразрез с общепринятым общественным мнением &quot; .\\n\\nПредседатель ОВЦС заверил , что у Русской православной церкви нет &quot; специально враждебного отношения к Западу &quot; , а когда возникает критика Запада , то прежде всего речь идет о &quot; навязывании либеральных ценностей &quot; .\\n&quot; Это представления о том , что семья не должна защищаться государством , что у человека могут быть такие сексуальные связи , какие он хочет , и столько , сколько он хочет , что семья - не обязательно союз мужчины и женщины , что детей можно отдавать на усыновление нетрадиционным семьям &quot; ,\\n- пояснил он .\\n\\nМитрополит Иларион также прокомментировал закон об оскорблении чувств верующих , отметив , что &quot; никто никому не должен запрещать свободу высказываний : если вам не нравится Церковь , ее учение или поведение отдельных членов , вы можете об этом говорить .\\nНо если происходит акт сознательного кощунства по отношению к священному для той или иной религии символу или в кощунственном свете выставляется образ дорогого для верующих человека или божественного лица , то может вступить в силу этот закон &quot; .\\nЭтот закон , по мнению митрополита Волоколамского , в России действительно необходим :\\n&quot; Мы живем в особой стране , где на протяжении веков мирно сосуществуют разные религии и конфессии .\\nЭто достаточно хрупкий мир , его можно взорвать неосторожными действиями или высказываниями , неумной внутренней политикой .\\nПоэтому , чтобы сохранить этот мир , мы все должны приложить особые усилия &quot; .'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = random.sample(texts, 1)[0]\n",
    "referencing = eval_referer(sample_text)\n",
    "sample_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Глава Отдела внешних церковных связей Московского патриархата , митрополит Волоколамский Иларион ( Алфеев ) gрокомментировал заявления социологов о росте популярности Иосифа Сталина у жителей России . Митрополит Иларион также прокомментировал закон об оскорблении чувств верующих , отметив , что &quot; никто никому не должен запрещать свободу высказываний : если вам не нравится Церковь , ее учение или поведение отдельных членов , вы можете об этом говорить . Этот закон , по мнению митрополита Волоколамского , в России действительно необходим : &quot; Мы живем в особой стране , где на протяжении веков мирно сосуществуют разные религии и конфессии .'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "referencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "654"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(referencing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:21<00:00,  9.10it/s]\n"
     ]
    }
   ],
   "source": [
    "json.dump([eval_referer(text) for text in tqdm(texts)], open(OUTPUT, 'w'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
